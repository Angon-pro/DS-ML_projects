{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c5d7c8-aa87-44a2-8d94-2426e8f2466a",
   "metadata": {},
   "source": [
    "# Deep Learning for Business Applications course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f62655-2e46-4de8-95ed-ceb1f578baba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TOPIC 5: Object detection problem. YOLO training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73aa981-0f41-4a8e-b523-0d3e3d4b761b",
   "metadata": {},
   "source": [
    "### 1. Libraries and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa90d67-0ed9-4eae-a585-f61fc22c09ee",
   "metadata": {},
   "source": [
    "#### 1.1. Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04217bb0-3144-443f-a9f6-c7f92a7d26b9",
   "metadata": {},
   "source": [
    "[Streamlit](https://streamlit.io/) is a framework that offers a faster way to build and share data applications. It helps you to turn data scripts into shareable web apps in minutes. It is written in pure Python and does not require front‑end experience to work with. Installation is very simple in our environment. Just use terminal or type here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908bb1f7-311f-4021-8540-ac1b4da2694a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /opt/conda/lib/python3.10/site-packages (1.40.2)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (4.2.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (5.2.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (8.1.3)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /opt/conda/lib/python3.10/site-packages (from streamlit) (1.25.2)\n",
      "Requirement already satisfied: packaging<25,>=20 in /opt/conda/lib/python3.10/site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (1.5.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (9.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /opt/conda/lib/python3.10/site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/conda/lib/python3.10/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (13.8.1)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /opt/conda/lib/python3.10/site-packages (from streamlit) (5.0.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/conda/lib/python3.10/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit) (6.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2022.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2022.12.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.14.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34823bc0-94e4-44b9-a0d7-96702d08e99e",
   "metadata": {},
   "source": [
    "#### 1.2. Other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d390f276-4198-42cd-aa1c-d3c7a70d0f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: ultralytics in /opt/conda/lib/python3.10/site-packages (8.3.38)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\n",
      "Requirement already satisfied: deepface in /opt/conda/lib/python3.10/site-packages (0.0.93)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python) (1.25.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.6.2)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.9.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.4.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.64.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.4)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.5.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.0.12)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: gdown>=3.10.1 in /opt/conda/lib/python3.10/site-packages (from deepface) (5.2.0)\n",
      "Requirement already satisfied: tensorflow>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from deepface) (2.10.0)\n",
      "Requirement already satisfied: keras>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from deepface) (2.10.0)\n",
      "Requirement already satisfied: Flask>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from deepface) (3.1.0)\n",
      "Requirement already satisfied: flask-cors>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from deepface) (5.0.0)\n",
      "Requirement already satisfied: mtcnn>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from deepface) (1.0.0)\n",
      "Requirement already satisfied: retina-face>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from deepface) (0.0.17)\n",
      "Requirement already satisfied: fire>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from deepface) (0.6.0)\n",
      "Requirement already satisfied: gunicorn>=20.1.0 in /opt/conda/lib/python3.10/site-packages (from deepface) (23.0.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire>=0.4.0->deepface) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire>=0.4.0->deepface) (2.3.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /opt/conda/lib/python3.10/site-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=1.1.2->deepface) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from Flask>=1.1.2->deepface) (8.1.3)\n",
      "Requirement already satisfied: blinker>=1.9 in /opt/conda/lib/python3.10/site-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=3.10.1->deepface) (4.11.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.4.2 in /opt/conda/lib/python3.10/site-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
      "Requirement already satisfied: lz4>=4.3.3 in /opt/conda/lib/python3.10/site-packages (from mtcnn>=0.1.0->deepface) (4.3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2022.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (1.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (22.12.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (3.7.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (65.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (1.47.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (2.10.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2.8.8)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.6.68)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.38.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (2.1.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.9.0->deepface) (2.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.9.0->deepface) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.9.0->deepface) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.9.0->deepface) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow>=1.9.0->deepface) (1.8.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.3.2.post1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.2.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=1.9.0->deepface) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=1.9.0->deepface) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=1.9.0->deepface) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow>=1.9.0->deepface) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=1.9.0->deepface) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow>=1.9.0->deepface) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python ultralytics transformers deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d80cba5-41d0-4279-b266-1dd16b663878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de48cf-d797-4cae-819c-69a586b839ee",
   "metadata": {},
   "source": [
    "#### 1.3. Free space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "463e25f1-3fa9-40c1-b59d-b7ac662dce5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmpfs                                      64M     0   64M   0% /dev\n",
      "/dev/vdg                                   12G  2.4G  9.4G  20% /home/jovyan\n",
      "/dev/vda2                                  95G   53G   39G  58% /etc/hosts\n",
      "shm                                        64M     0   64M   0% /dev/shm\n"
     ]
    }
   ],
   "source": [
    "!df -h | grep dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0334c25-dfff-460b-b521-46dd8952b3be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\n",
      "4 drwxr-sr-x 6 jovyan users 4096 Nov 27 17:41 models--openai--clip-vit-base-patch16\n",
      "4 drwxr-sr-x 6 jovyan users 4096 Nov 27 17:41 models--Salesforce--blip-image-captioning-base\n",
      "4 -rw-r--r-- 1 jovyan users    1 Nov 27 17:41 version.txt\n",
      "ls: cannot access '/home/jovyan/.cache/torch/hub/': No such file or directory\n",
      "total 566504\n",
      "drwxr-sr-x 2 jovyan users      4096 Nov 27 17:43 .\n",
      "drwxr-sr-x 3 jovyan users      4096 Nov 27 17:41 ..\n",
      "-rw-r--r-- 1 jovyan users 580085408 Nov 27 17:43 vgg_face_weights.h5\n"
     ]
    }
   ],
   "source": [
    "# a cache dir for Huggin Face Hub models\n",
    "!ls -ls ~/.cache/huggingface/hub\n",
    "\n",
    "# a cache dir for PyTorch models\n",
    "!ls -ls ~/.cache/torch/hub/\n",
    "\n",
    "# a cache dir for DeepFace models\n",
    "!ls -la ~/.deepface/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719e8946-ab4e-411c-a0e9-bbc90d29695a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use `rm -rf` !!! WITH CARE !!!\n",
    "\n",
    "!rm -rf ~/.cache/huggingface/hub\n",
    "!rm -rf ~/.cache/torch/hub/checkpoints\n",
    "!rm -rf ~/.deepface/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d1367-0a9e-4f7c-bb8f-b798ce52d5a6",
   "metadata": {},
   "source": [
    "### 2. How Streamlit works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb7058-ad37-458b-b278-044806419a31",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Main concepts](https://docs.streamlit.io/library/get-started/main-concepts) require you to create a normal Python script with all necessary elements for your future app and run it with `streamlit run` like `streamlit run your_script.py [-- script args]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e1028-ada6-46e6-85e9-002086c6f6f7",
   "metadata": {},
   "source": [
    "#### 2.1. Python script with app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa5935-7b74-4b17-b664-7113b6d8a469",
   "metadata": {},
   "source": [
    "Streamlit's architecture allows you to write apps the same way you write plain Python scripts. Let's create the sample script with `%%writefile` magic command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3877b6c-2817-4f68-8825-419a8f8a99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "# Title of our demo app\n",
    "st.title('Meet the first Streamlit application')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74095332-886b-4cda-8169-a0642cd53aab",
   "metadata": {},
   "source": [
    "#### 2.2. Run application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d23de-7527-44bb-a4ff-a505f7fa7acb",
   "metadata": {},
   "source": [
    "Run application is very easy. Just open a terminal in the folder with your Python script `stapp.py` and type:\n",
    "\n",
    "`streamlit run stapp.py --server.port 20000 --browser.gatherUsageStats False` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8951f3ce-28a5-45a9-ac31-e04fbbe002a5",
   "metadata": {},
   "source": [
    "Your Streamlit application will be available with the following URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d15b55-a38a-4888-90d8-b0b26aa99f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit available at: https://jhas01.gsom.spbu.ru/user/st136973/proxy/20000/\n"
     ]
    }
   ],
   "source": [
    "print('Streamlit available at:',\n",
    "      'https://jhas01.gsom.spbu.ru{}proxy/{}/'.format(\n",
    "          os.environ['JUPYTERHUB_SERVICE_PREFIX'], 20000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6498d5c1-a3fb-4050-bc78-b102d8668511",
   "metadata": {},
   "source": [
    "#### 2.2. Basic examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c63dd-c203-4952-ad67-f29d478a9d43",
   "metadata": {},
   "source": [
    "##### 2.2.1. Nice headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd66dbb-36fa-433d-829c-9a87340dd160",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.header('Nice looking header string', divider='rainbow')\n",
    "st.header('_Here is header under the line_ :fire:')\n",
    "\n",
    "st.subheader('Subheader is also here', divider='rainbow')\n",
    "st.subheader(':blue[_We like Streamlit_] :star:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb65cc-d528-48fe-9905-5eab89d38ab8",
   "metadata": {},
   "source": [
    "##### 2.2.2. Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334000c-10bb-449c-a2f7-9b1bdf6dda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.header('Just a header', divider='rainbow')\n",
    "st.text('Just a text under the header')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f29cf-9af2-4687-a3fd-8639e809c3f4",
   "metadata": {},
   "source": [
    "##### 2.2.3. Write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d83f88-dad4-477a-a817-23bf578f7fe3",
   "metadata": {},
   "source": [
    "Along with magic commands, `st.write()` is Streamlit's \"Swiss Army knife\". You can pass almost anything to `st.write()`: text, data, Matplotlib figures, charts and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fbd00-8d22-4b2e-9adc-e5b72090108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "st.header('Demo of write function', divider='rainbow')\n",
    "st.subheader('Table and plot at one application')\n",
    "\n",
    "st.divider()\n",
    "\n",
    "st.write(\"Here's demo table from the dataframe:\")\n",
    "fruits_data = pd.DataFrame(\n",
    "    {\n",
    "        'fruits': ['apple', 'peach', 'pineapple', 'watermelon'],\n",
    "        'color': ['green', 'orange', 'yellow', 'stripes'],\n",
    "        'weight': [1, 2, 5, 10]\n",
    "    }\n",
    ")\n",
    "st.write(fruits_data)\n",
    "\n",
    "st.divider()\n",
    "\n",
    "st.write(\"Here's demo chart for fruits:\")\n",
    "chart_data = pd.DataFrame(\n",
    "     np.random.randn(20, 4),\n",
    "     columns=['apple', 'peach', 'pineapple', 'watermelon']\n",
    ")\n",
    "st.line_chart(chart_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccde3d-f9a5-4ac6-bbdd-faa65c5d82b9",
   "metadata": {},
   "source": [
    "### 3. AI with Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959f5da-09e9-4109-99fa-db3da3ac1407",
   "metadata": {},
   "source": [
    "#### 3.1. Upload pipeline for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80d0fa-f4d0-4da9-8ce2-0989032f70b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and plot image')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        bytes_data = uploaded_file.read()\n",
    "        img = Image.open(io.BytesIO(bytes_data))\n",
    "        st.divider()\n",
    "        st.image(img, caption='Uploaded image')\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f889616e-5f2e-4ff2-8a92-c9928b93ceb0",
   "metadata": {},
   "source": [
    "#### 3.2. Add some AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2510ba-77f7-4872-96e4-51765bd3bcf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# use of AI model for image captioning\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration\n",
    ")\n",
    "\n",
    "\n",
    "def img_caption(model, processor, img, text=None):\n",
    "    \"\"\"\n",
    "    Uses BLIP model to caption image.\n",
    "    \n",
    "    \"\"\"\n",
    "    res = None\n",
    "    if text:\n",
    "        # conditional image captioning\n",
    "        inputs = processor(img, text, return_tensors='pt')\n",
    "    else:\n",
    "        # unconditional image captioning\n",
    "        inputs = processor(img, return_tensors='pt')\n",
    "    out = model.generate(**inputs)\n",
    "    res = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "with st.spinner('Please wait, application is initializing...'):\n",
    "    MODEL_CAP_NAME = 'Salesforce/blip-image-captioning-base'\n",
    "    PROCESSOR_CAP = BlipProcessor.from_pretrained(MODEL_CAP_NAME)\n",
    "    MODEL_CAP = BlipForConditionalGeneration.from_pretrained(MODEL_CAP_NAME)\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and plot image with AI caption')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        # input text for conditional image captioning\n",
    "        text = st.text_input(\n",
    "            'Input text for conditional image captioning (if needed)', \n",
    "            ''\n",
    "        )\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # use image caption model for uploaded image\n",
    "            caption = img_caption(\n",
    "                model=MODEL_CAP, \n",
    "                processor=PROCESSOR_CAP, \n",
    "                img=img, \n",
    "                text=text\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img, caption=caption)\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0a903-603a-4e72-bb05-f687aee8fef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Use of AI model for image captioning\n",
    "# and object detection at the image\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration\n",
    ")\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "def img_caption(model, processor, img, text=None):\n",
    "    \"\"\"\n",
    "    Uses BLIP model to caption image.\n",
    "    \n",
    "    \"\"\"\n",
    "    res = None\n",
    "    if text:\n",
    "        # conditional image captioning\n",
    "        inputs = processor(img, text, return_tensors='pt')\n",
    "    else:\n",
    "        # unconditional image captioning\n",
    "        inputs = processor(img, return_tensors='pt')\n",
    "    out = model.generate(**inputs)\n",
    "    res = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "def img_detect(model, img, plot=False):\n",
    "    \"\"\"\n",
    "    Run YOLO inference on an image.\n",
    "    \n",
    "    \"\"\"\n",
    "    result = model(img)[0]\n",
    "    boxes = result.boxes  # boxes object for bounding box outputs\n",
    "    names = model.names\n",
    "    objs = []\n",
    "    for c, p in zip(boxes.cls, boxes.conf):\n",
    "        objs.append({names[int(c)]: p.item()})\n",
    "    img_bgr = result.plot()  # BGR-order numpy array\n",
    "    img_rgb = Image.fromarray(img_bgr[..., ::-1])  # RGB-order PIL image\n",
    "    if plot:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.show()\n",
    "    return objs, img_rgb\n",
    "\n",
    "\n",
    "with st.spinner('Please wait, application is initializing...'):\n",
    "    MODEL_CAP_NAME = 'Salesforce/blip-image-captioning-base'\n",
    "    PROCESSOR_CAP = BlipProcessor.from_pretrained(MODEL_CAP_NAME)\n",
    "    MODEL_CAP = BlipForConditionalGeneration.from_pretrained(MODEL_CAP_NAME)\n",
    "\n",
    "    MODEL_DET_NAME = 'yolov8n.pt'\n",
    "    MODEL_DET = YOLO(MODEL_DET_NAME)\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and plot image with AI caption and YOLO detection')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        # input text for conditional image captioning\n",
    "        text = st.text_input(\n",
    "            'Input text for conditional image captioning (if needed)', \n",
    "            ''\n",
    "        )\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # image caption model for uploaded image\n",
    "            caption = img_caption(\n",
    "                model=MODEL_CAP, \n",
    "                processor=PROCESSOR_CAP, \n",
    "                img=img, \n",
    "                text=text\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img, caption=caption)\n",
    "            \n",
    "            # object detection model for uploaded image\n",
    "            objs, img_det = img_detect(\n",
    "                model=MODEL_DET, \n",
    "                img=img\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img_det, caption='object detection', width=800)\n",
    "            st.divider()\n",
    "            st.caption('Objects dictionary:')\n",
    "            st.write(objs)\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9692f84-e389-44e2-b325-be19fd9cb045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Use of AI model for image captioning\n",
    "# and object detection at the image\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def zeroshot(classifier, classes, img):\n",
    "    scores = classifier(\n",
    "        img,\n",
    "        candidate_labels=classes\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "with st.spinner('Please wait, application is initializing...'):\n",
    "    MODEL_ZERO_NAME = 'openai/clip-vit-base-patch16'\n",
    "    CLASSIFIER_ZERO = pipeline('zero-shot-image-classification', model=MODEL_ZERO_NAME)\n",
    "    CLASSES = [\n",
    "        'a photo of nature',\n",
    "        'a photo of cat',\n",
    "        'a photo of a party',\n",
    "        'a photo of a food'\n",
    "    ]\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and classifying it with zero-shot')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # classifying image with zero-shot modele\n",
    "            scores = zeroshot(\n",
    "                classifier=CLASSIFIER_ZERO, \n",
    "                classes=CLASSES, \n",
    "                img=img\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img, caption='zero-shot classification')\n",
    "            \n",
    "            # plot a diagram ith scores and scores output\n",
    "            st.divider()\n",
    "            df = pd.DataFrame(scores)\n",
    "            df = df.set_index('label')\n",
    "            st.bar_chart(df)\n",
    "            st.divider()\n",
    "            st.caption('Scores dictionary:')\n",
    "            st.write(scores)\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5a314-6294-4005-8957-b8de29984ece",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.3. Add some OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78889b0e-e7a7-47a2-bc7b-450217f431f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Use of OCR model for text extracting \n",
    "# from image or PDF file\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_bytes\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def pdf2img(pdf_bytes):\n",
    "    \"\"\"\n",
    "    Turns pdf file to set of jpeg images.\n",
    "\n",
    "    \"\"\"\n",
    "    images = convert_from_bytes(pdf_bytes.read())\n",
    "    return images\n",
    "\n",
    "\n",
    "def ocr_text(img, lang='eng'):\n",
    "    \"\"\"\n",
    "    Takes the text from image.\n",
    "    \n",
    "    :lang: language is `eng` by default,\n",
    "           use `eng+rus` for two languages in document\n",
    "\n",
    "    \"\"\"\n",
    "    text = str(pytesseract.image_to_string(\n",
    "        img,\n",
    "        lang=lang\n",
    "    ))\n",
    "    return text\n",
    "\n",
    "\n",
    "def ocr_text_dir(img_dir, lang='eng'):\n",
    "    \"\"\"\n",
    "    Takes the text from images in a folder.\n",
    "\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for img_name in tqdm(sorted(os.listdir(img_dir))):\n",
    "        if '.jpg' in img_name:\n",
    "            img = Image.open(f'{IMG_PATH}/{img_name}')\n",
    "            text_tmp = ocr_text(img, lang=lang)\n",
    "            text = ' '.join([text, text_tmp])\n",
    "    return text\n",
    "\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and extracting text from it')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you file or image')\n",
    "uploaded_file = st.file_uploader('Select a file (JPEG or PDF)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    lang = st.selectbox(\n",
    "            'Select language to extract ',\n",
    "            ('eng', 'rus', 'eng+rus')\n",
    "        )\n",
    "    if '.jpg' in file_name:\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # image caption model for uploaded image\n",
    "            text = ocr_text(img, lang=lang)\n",
    "            st.divider()\n",
    "            st.write('#### Text extracted')\n",
    "            st.write(text)\n",
    "    elif '.pdf' in file_name:\n",
    "        with st.spinner('Please wait...'):\n",
    "            imgs = pdf2img(uploaded_file)\n",
    "            text = ''\n",
    "            for img in imgs:\n",
    "                text_tmp = ocr_text(img, lang=lang)\n",
    "                text = ' '.join([text, text_tmp])\n",
    "            st.divider()\n",
    "            st.write('#### Text extracted')\n",
    "            st.write(text)\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca128d06-5fe3-48ab-a6b7-6b6bea0ac856",
   "metadata": {},
   "source": [
    "### 3.4. Add some faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900be527-4853-46a3-86e3-13c96e64ec7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile stapp.py\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Use of DeepFace framework\n",
    "# for faces detection and recognotion\n",
    "\n",
    "import io\n",
    "import cv2\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from deepface import DeepFace\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def zeroshot(classifier, classes, img):\n",
    "    scores = classifier(\n",
    "        img,\n",
    "        candidate_labels=classes\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "with st.spinner('Please wait, application is initializing...'):\n",
    "    MODEL_ZERO_NAME = 'openai/clip-vit-base-patch16'\n",
    "    CLASSIFIER_ZERO = pipeline('zero-shot-image-classification', model=MODEL_ZERO_NAME)\n",
    "    CLASSES = [\n",
    "        'a photo of nature',\n",
    "        'a photo of cat',\n",
    "        'a photo of a party',\n",
    "        'a photo of a food'\n",
    "    ]\n",
    "    DEEPFACE_MODELS = [\n",
    "        'VGG-Face',\n",
    "        'Facenet',\n",
    "        'Facenet512',\n",
    "        'OpenFace',\n",
    "        'DeepFace',\n",
    "        'DeepID',\n",
    "        'ArcFace',\n",
    "        'Dlib',\n",
    "        'SFace',\n",
    "        'GhostFaceNet'\n",
    "    ]\n",
    "    DB_PATH = '/home/jovyan/dlba/topic_09/app/data/db'\n",
    "\n",
    "st.header('Demo of image uploading', divider='rainbow')\n",
    "st.subheader('Uploading file and classifying it with zero-shot and face recognition')\n",
    "st.divider()\n",
    "\n",
    "st.write('#### Upload you image')\n",
    "uploaded_file = st.file_uploader('Select an image file (JPEG format)')\n",
    "if uploaded_file is not None:\n",
    "    file_name = uploaded_file.name\n",
    "    if '.jpg' in file_name:\n",
    "        with st.spinner('Please wait...'):\n",
    "            bytes_data = uploaded_file.read()\n",
    "            img = Image.open(io.BytesIO(bytes_data))\n",
    "            \n",
    "            # classifying image with zero-shot modele\n",
    "            scores = zeroshot(\n",
    "                classifier=CLASSIFIER_ZERO, \n",
    "                classes=CLASSES, \n",
    "                img=img\n",
    "            )\n",
    "            st.divider()\n",
    "            st.image(img, caption='zero-shot classification')\n",
    "            \n",
    "            # plot a diagram ith scores and scores output\n",
    "            st.divider()\n",
    "            df = pd.DataFrame(scores)\n",
    "            df = df.set_index('label')\n",
    "            st.bar_chart(df)\n",
    "            st.divider()\n",
    "            st.caption('Scores dictionary:')\n",
    "            st.write(scores)\n",
    "            \n",
    "            # faces detection and recognition\n",
    "            results = DeepFace.find(\n",
    "                img_path=np.array(img),  # face to find\n",
    "                db_path=f'{DB_PATH}',  # path to directory with faces\n",
    "                model_name=DEEPFACE_MODELS[0],\n",
    "                enforce_detection=False\n",
    "            )\n",
    "            st.divider()\n",
    "            st.caption('Faces recognition:')\n",
    "            found = []\n",
    "            for result in results:\n",
    "                name = result.identity.values\n",
    "                if name:\n",
    "                    found.append(\n",
    "                        name[0].replace(f'{DB_PATH}/', '').replace('.jpg', '')\n",
    "                    )\n",
    "            if found:\n",
    "                st.write(f'Found: {\" ,\".join(found)}')\n",
    "            else:\n",
    "                st.write('No known faces found')\n",
    "    else:\n",
    "        st.error('File read error', icon='⚠️')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45be59-3147-4116-b8f0-9ab4555ea8a7",
   "metadata": {},
   "source": [
    "## 4. Move to developing app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd486b-d1a3-4f75-a181-372e0ee2e9b7",
   "metadata": {},
   "source": [
    "Let's get out Jupyter notebooks to a hardcore development process..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119322e9-8ec7-46c3-9352-1a803f1eadb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <font color='red'>HOME ASSIGNMENT (Final project)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81d902-323d-4dc5-997e-420d234b2e04",
   "metadata": {},
   "source": [
    "Your final project tasks are:\n",
    "1. Enlarge number of categories for uploaded images classification\n",
    "2. Change YOLO detection to segmentation model (use new version of YOLO, version 11 is released)\n",
    "3. Apply emotion detector model and add emotions for your friends faces recognition pipeline\n",
    "4. Implement application's page for OCR with Tesseract library\n",
    "5. Add text summarization option for text extracted in OCR page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495287f6-5d38-4ecf-9c81-2afe33fa639b",
   "metadata": {},
   "source": [
    "<font color='red'>Use application code snippets for the Final project, not the notebooks!</font> To run application use the following command from the terminal:\n",
    "\n",
    "`streamlit run Main_page.py --server.port 20000 --browser.gatherUsageStats False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9aaec1-3e7f-4590-983b-04c2aa5245f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
